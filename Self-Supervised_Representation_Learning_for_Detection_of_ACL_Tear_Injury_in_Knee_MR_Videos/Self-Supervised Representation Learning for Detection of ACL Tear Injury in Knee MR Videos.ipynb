{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4DW6a6Y8hWtT"
   },
   "source": [
    "# Imports and Utility functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AYyIZ6dphbN8"
   },
   "source": [
    "## <font color='orange'>Imports</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "yQrK-eB3mBAC",
    "outputId": "9fdd9b1f-94bb-41ff-c0ed-e24cd28a1eb9"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shutil, time, os, requests, random, copy\n",
    "from itertools import permutations \n",
    "import seaborn as sns\n",
    "import imageio\n",
    "from skimage.transform import rotate, AffineTransform, warp, resize\n",
    "#from google.colab.patches import cv2_imshow\n",
    "from IPython.display import clear_output, Image, SVG\n",
    "import h5py\n",
    "\n",
    "#%tensorflow_version 2.x\n",
    "#%load_ext tensorboard\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
    "\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPool2D, GlobalAveragePooling2D, AveragePooling2D, BatchNormalization, Reshape\n",
    "\n",
    "#from tensorflow.keras.layers import Conv3D, MaxPooling3D, GlobalAveragePooling3D, AveragePooling3D\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Activation, Concatenate, Lambda, LeakyReLU\n",
    "\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras import regularizers, activations\n",
    "from tensorflow.keras.utils import to_categorical, Sequence\n",
    "\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc, f1_score\n",
    "from sklearn.metrics import average_precision_score, precision_recall_curve, roc_auc_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "#import matplotlib.animation as animation\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TpMRrcKfXsbj"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.inception_resnet_v2 import preprocess_input as ppi_irv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "id": "kvZYdDkGhd6o"
   },
   "source": [
    "## <font color='orange'>Downloading and Extracting Data</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "hidden": true,
    "id": "QdHt9DS_pOA6",
    "outputId": "91998c21-f19f-4b31-e3d2-4a217c70ab72"
   },
   "outputs": [],
   "source": [
    "!wget http://download.cs.stanford.edu/deep/MRNet-v1.0.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "hidden": true,
    "id": "MvBCkttepOxO",
    "outputId": "bdad1c76-3ea0-4c02-93a8-7a7eba068efd"
   },
   "outputs": [],
   "source": [
    "!unzip MRNet-v1.0.zip -d ~/MRNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "hidden": true,
    "id": "pZRmUYkcpRYz",
    "outputId": "4e829bb4-a6bd-4ead-9692-b47c1ca978a3"
   },
   "outputs": [],
   "source": [
    "!ls ~/MRNet/MRNet-v1.0/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "za2dO2s5kUGd"
   },
   "source": [
    "## <font color='orange'>Analysing and Cleaning Data</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OSgqPrYakUGd"
   },
   "source": [
    "### Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls ~/MRNet-v1/MRNet-v1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "vGFn5npjkUGe",
    "outputId": "f5b06467-835f-4a5a-a6df-4189fbbc777b"
   },
   "outputs": [],
   "source": [
    "mrnet_path = '~/MRNet-v1/MRNet-v1.0'\n",
    "contents = os.listdir(mrnet_path)\n",
    "print(contents)\n",
    "print('\\nLabel Files...')\n",
    "label_files = [x for x in contents if x.endswith('.csv')]\n",
    "print(label_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mhLpiTOxpabA"
   },
   "source": [
    "### Real Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "anVc7J0EpVTe"
   },
   "outputs": [],
   "source": [
    "#For Colab /root/MRNet/MRNet-v1.0/\n",
    "trabn = pd.read_csv(mrnet_path+'/'+'train-abnormal.csv',header=None)\n",
    "#trabn.head()\n",
    "tracl = pd.read_csv(mrnet_path+'/'+'train-acl.csv',header=None)\n",
    "#tracl.head()\n",
    "trmen = pd.read_csv(mrnet_path+'/'+'train-meniscus.csv',header=None)\n",
    "#trmen.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w_h49rgokUGr"
   },
   "outputs": [],
   "source": [
    "trabn.columns = ['patient_id','label']\n",
    "tracl.columns = ['patient_id','label']\n",
    "trmen.columns = ['patient_id','label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "LFLSUNlfkUGt",
    "outputId": "2ffc4c14-edc7-4489-d9ce-b738db5e0421"
   },
   "outputs": [],
   "source": [
    "tr_multilabel = trabn.merge(tracl,on='patient_id').merge(trmen,on='patient_id')\n",
    "tr_multilabel.columns = ['patient_id','abn','acl','men']\n",
    "tr_multilabel.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qcBYWgQXpeOh"
   },
   "outputs": [],
   "source": [
    "#For Colab /root/MRNet/MRNet-v1.0/\n",
    "valabn = pd.read_csv(mrnet_path+'/'+'valid-abnormal.csv',header=None)\n",
    "valacl = pd.read_csv(mrnet_path+'/'+'valid-acl.csv',header=None)\n",
    "valmen = pd.read_csv(mrnet_path+'/'+'valid-meniscus.csv',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HCFB7iPtxFkB"
   },
   "outputs": [],
   "source": [
    "valabn.columns = ['patient_id','label']\n",
    "valacl.columns = ['patient_id','label']\n",
    "valmen.columns = ['patient_id','label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "colab_type": "code",
    "id": "7VkI9BLokcZn",
    "outputId": "401af580-8cf9-4973-9d77-9011ebaa5164"
   },
   "outputs": [],
   "source": [
    "val_multilabel = valabn.merge(valacl,on='patient_id').merge(valmen,on='patient_id')\n",
    "val_multilabel.columns = ['patient_id','abn','acl','men']\n",
    "val_multilabel.head(120)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8Q4ocHEjkUGy"
   },
   "source": [
    "### <font color='blue'>Filename DataFrame</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "colab_type": "code",
    "id": "KJ2-QRfnkUGy",
    "outputId": "5cd5ad2e-629a-4be2-bc73-10521de1411d"
   },
   "outputs": [],
   "source": [
    "tr_filenames_df = pd.DataFrame(columns=['filename'])\n",
    "tr_filenames_df['filename'] = os.listdir(mrnet_path+'/train/'+'axial')\n",
    "tr_filenames_df['patient_id'] = tr_filenames_df.apply(lambda x : int(x['filename'][:-4]),axis=1)\n",
    "tr_filenames_df = tr_filenames_df[list(('patient_id','filename'))]\n",
    "tr_filenames_df.sort_values(by=['patient_id'],ascending=True,inplace=True,ignore_index=True)\n",
    "\n",
    "tr_filenames_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "colab_type": "code",
    "id": "xYkMwRwNqfe0",
    "outputId": "eb3664d1-61f3-450a-a57d-8674b87f472f"
   },
   "outputs": [],
   "source": [
    "val_filenames_df = pd.DataFrame(columns=['filename'])\n",
    "val_filenames_df['filename'] = os.listdir(mrnet_path+'/valid/'+'axial')\n",
    "val_filenames_df['patient_id'] = val_filenames_df.apply(lambda x : int(x['filename'][:-4]),axis=1)\n",
    "val_filenames_df = val_filenames_df[list(('patient_id','filename'))]\n",
    "val_filenames_df.sort_values(by=['patient_id'],ascending=True,inplace=True,ignore_index=True)\n",
    "\n",
    "val_filenames_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "id": "9Y2cdtkeps4I"
   },
   "source": [
    "## <font color='orange'>Visualizing the Data</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 337
    },
    "colab_type": "code",
    "hidden": true,
    "id": "wFXR0XGoM80A",
    "outputId": "0fdeba2d-d4b5-4007-df2b-d61d9cc797dc"
   },
   "outputs": [],
   "source": [
    "tracl.iloc[:,1].hist(figsize = (10, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "hidden": true,
    "id": "YtiuYqZebmls",
    "outputId": "6fcdc8ad-3bb9-4d96-f0f0-03e45a115833"
   },
   "outputs": [],
   "source": [
    "np.count_nonzero(tracl.iloc[:,1]==1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yzDaX73PhzXv"
   },
   "source": [
    "## <font color='orange'>Utility Functions</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8c44LNulTb8i"
   },
   "source": [
    "## <font color='blue'>Declaring the required PATH variables</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x6u18pYcWtfw"
   },
   "outputs": [],
   "source": [
    "#For Colab '/root/MRNet/MRNet-v1.0/'\n",
    "\n",
    "train_dir = mrnet_path+'/train'\n",
    "valid_dir = mrnet_path+'/valid'\n",
    "axial_mode= 'axial'\n",
    "sagit_mode='sagittal'\n",
    "coron_mode='coronal'\n",
    "base_dir = mrnet_path\n",
    "\n",
    "NUM_FRAMES = 1\n",
    "batch_size = 32 #32\n",
    "NUM_CLASSES = 1000 #3\n",
    "NUM_PATCHES = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "odE4wSWawzrC"
   },
   "source": [
    "## <font color='blue'>Callbacks</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Klg4fy0cMW_U"
   },
   "outputs": [],
   "source": [
    "#os.makedirs('saved_models/')\n",
    "\n",
    "def get_callbacks(pord,acctype):\n",
    "    save_dir = '/saved_models/'\n",
    "    model_name = 'sagittal_' + pord + '_best_model.h5'\n",
    "\n",
    "    checkpoint = tf.keras.callbacks.ModelCheckpoint(save_dir+model_name, \n",
    "                                                    monitor = 'val_' + acctype + '_accuracy', verbose=1, \n",
    "                                                    save_best_only=True, mode='max')\n",
    "\n",
    "    #reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_accuracy', factor=0.5, patience=4, \n",
    "    #                               verbose=1, mode='max', min_lr=0.00001)\n",
    "                              \n",
    "    #early = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=5)\n",
    "\n",
    "    callbacks_list = [checkpoint]\n",
    "\n",
    "    return callbacks_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "42RiWwJZkUH5"
   },
   "source": [
    "## <font color='blue'>Performance Metrics</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kpT8np6skUH5"
   },
   "outputs": [],
   "source": [
    "#util_wk2\n",
    "def TP(y, pred, th=0.5):\n",
    "    pred_t = (pred > th)\n",
    "    return np.sum((pred_t == True) & (y == 1))\n",
    "\n",
    "\n",
    "def TN(y, pred, th=0.5):\n",
    "    pred_t = (pred > th)\n",
    "    return np.sum((pred_t == False) & (y == 0))\n",
    "\n",
    "\n",
    "def FN(y, pred, th=0.5):\n",
    "    pred_t = (pred > th)\n",
    "    return np.sum((pred_t == False) & (y == 1))\n",
    "\n",
    "\n",
    "def FP(y, pred, th=0.5):\n",
    "    pred_t = (pred > th)\n",
    "    return np.sum((pred_t == True) & (y == 0))\n",
    "\n",
    "def get_accuracy(y, pred, th=0.5):\n",
    "    tp = TP(y,pred,th)\n",
    "    fp = FP(y,pred,th)\n",
    "    tn = TN(y,pred,th)\n",
    "    fn = FN(y,pred,th)\n",
    "    \n",
    "    return (tp+tn)/(tp+fp+tn+fn)\n",
    "\n",
    "def get_prevalence(y):\n",
    "    return np.sum(y)/y.shape[0]\n",
    "\n",
    "def sensitivity(y, pred, th=0.5):\n",
    "    tp = TP(y,pred,th)\n",
    "    fn = FN(y,pred,th)\n",
    "    \n",
    "    return tp/(tp+fn)\n",
    "\n",
    "def specificity(y, pred, th=0.5):\n",
    "    tn = TN(y,pred,th)\n",
    "    fp = FP(y,pred,th)\n",
    "    \n",
    "    return tn/(tn+fp)\n",
    "\n",
    "def get_ppv(y, pred, th=0.5):\n",
    "    tp = TP(y,pred,th)\n",
    "    fp = FP(y,pred,th)\n",
    "    \n",
    "    return tp/(tp+fp)\n",
    "\n",
    "def get_npv(y, pred, th=0.5):\n",
    "    tn = TN(y,pred,th)\n",
    "    fn = FN(y,pred,th)\n",
    "    \n",
    "    return tn/(tn+fn)\n",
    "\n",
    "\n",
    "def get_performance_metrics(y, pred, class_labels, tp=TP,\n",
    "                            tn=TN, fp=FP,\n",
    "                            fn=FN,\n",
    "                            acc=get_accuracy, prevalence=get_prevalence, \n",
    "                            spec=specificity,sens=sensitivity, ppv=get_ppv, \n",
    "                            npv=get_npv, auc=roc_auc_score, f1=f1_score,\n",
    "                            thresholds=[]):\n",
    "    if len(thresholds) != len(class_labels):\n",
    "        thresholds = [.5] * len(class_labels)\n",
    "\n",
    "    columns = [\"Injury\", \"TP\", \"TN\", \"FP\", \"FN\", \"Accuracy\", \"Prevalence\",\n",
    "               \"Sensitivity\",\n",
    "               \"Specificity\", \"PPV\", \"NPV\", \"AUC\", \"F1\", \"Threshold\"]\n",
    "    df = pd.DataFrame(columns=columns)\n",
    "    for i in range(len(class_labels)):\n",
    "        df.loc[i] = [class_labels[i],\n",
    "                     round(tp(y[:, i], pred[:, i]),3),\n",
    "                     round(tn(y[:, i], pred[:, i]),3),\n",
    "                     round(fp(y[:, i], pred[:, i]),3),\n",
    "                     round(fn(y[:, i], pred[:, i]),3),\n",
    "                     round(acc(y[:, i], pred[:, i], thresholds[i]),3),\n",
    "                     round(prevalence(y[:, i]),3),\n",
    "                     round(sens(y[:, i], pred[:, i], thresholds[i]),3),\n",
    "                     round(spec(y[:, i], pred[:, i], thresholds[i]),3),\n",
    "                     round(ppv(y[:, i], pred[:, i], thresholds[i]),3),\n",
    "                     round(npv(y[:, i], pred[:, i], thresholds[i]),3),\n",
    "                     round(auc(y[:, i], pred[:, i]),3),\n",
    "                     round(f1(y[:, i], pred[:, i] > thresholds[i]),3),\n",
    "                     round(thresholds[i], 3)]\n",
    "\n",
    "    df = df.set_index(\"Injury\")\n",
    "    return df\n",
    "\n",
    "def bootstrap_metric(y, pred, classes, metric='auc',bootstraps = 100, fold_size = 1000):\n",
    "    statistics = np.zeros((len(classes), bootstraps))\n",
    "    if metric=='AUC':\n",
    "        metric_func = roc_auc_score\n",
    "    if metric=='Sensitivity':\n",
    "        metric_func = sensitivity\n",
    "    if metric=='Specificity':\n",
    "        metric_func = specificity\n",
    "    if metric=='Accuracy':\n",
    "        metric_func = get_accuracy\n",
    "    for c in range(len(classes)):\n",
    "        df = pd.DataFrame(columns=['y', 'pred'])\n",
    "        df.loc[:, 'y'] = y[:, c]\n",
    "        df.loc[:, 'pred'] = pred[:, c]\n",
    "        # get positive examples for stratified sampling\n",
    "        df_pos = df[df.y == 1]\n",
    "        df_neg = df[df.y == 0]\n",
    "        prevalence = len(df_pos) / len(df)\n",
    "        for i in range(bootstraps):\n",
    "            # stratified sampling of positive and negative examples\n",
    "            pos_sample = df_pos.sample(n = int(fold_size * prevalence), replace=True)\n",
    "            neg_sample = df_neg.sample(n = int(fold_size * (1-prevalence)), replace=True)\n",
    "\n",
    "            y_sample = np.concatenate([pos_sample.y.values, neg_sample.y.values])\n",
    "            pred_sample = np.concatenate([pos_sample.pred.values, neg_sample.pred.values])\n",
    "            score = metric_func(y_sample, pred_sample)\n",
    "            statistics[c][i] = score\n",
    "    return statistics\n",
    "\n",
    "def get_confidence_intervals(y,pred,class_labels):\n",
    "    \n",
    "    metric_dfs = {}\n",
    "    for metric in ['AUC','Sensitivity','Specificity','Accuracy']:\n",
    "        statistics = bootstrap_metric(y,pred,class_labels,metric)\n",
    "        df = pd.DataFrame(columns=[\"Mean \"+metric+\" (CI 5%-95%)\"])\n",
    "        for i in range(len(class_labels)):\n",
    "            mean = statistics.mean(axis=1)[i]\n",
    "            max_ = np.quantile(statistics, .95, axis=1)[i]\n",
    "            min_ = np.quantile(statistics, .05, axis=1)[i]\n",
    "            df.loc[class_labels[i]] = [\"%.2f (%.2f-%.2f)\" % (mean, min_, max_)]\n",
    "        metric_dfs[metric] = df\n",
    "    return metric_dfs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GM2nQIr9kUIV"
   },
   "outputs": [],
   "source": [
    "def build_augmentations():\n",
    "    ROTATION = [-15,0,15]\n",
    "    TRANSLATEX = [-6,0,6]\n",
    "    TRANSLATEY = [-6,0,6]\n",
    "    SCALING = [1,1.15]\n",
    "    SHEAR = []\n",
    "    classes = {}\n",
    "    cind = 0\n",
    "    for rot in ROTATION:\n",
    "        for tranX in TRANSLATEX:\n",
    "            for tranY in TRANSLATEY:\n",
    "                for sc in SCALING:\n",
    "                    classes[cind] = [rot,tranX,tranY,sc]\n",
    "                    cind+=1\n",
    "    return classes\n",
    "\n",
    "\n",
    "augmentations = build_augmentations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 935
    },
    "colab_type": "code",
    "id": "hzqGnLXYWC5O",
    "outputId": "e346a2f7-daf0-4460-af27-011e82d301dc"
   },
   "outputs": [],
   "source": [
    "augmentations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8iThsv9s0Y1P"
   },
   "source": [
    "## Pretext Patch Prediction Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5b0qXt5D5U5y"
   },
   "outputs": [],
   "source": [
    "NUM_PATCHES = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GKROm-Tg0YRo"
   },
   "outputs": [],
   "source": [
    "def build_ppp_labels(num_patches):\n",
    "    ppp_labels_perms = list(permutations(range(0,num_patches)))\n",
    "    ppp_labels_perms = [list(t) for t in ppp_labels_perms]\n",
    "    #print(ppp_labels_perms)\n",
    "\n",
    "    ppp_labels = {}\n",
    "    label_num= 0\n",
    "    for key in ppp_labels_perms:\n",
    "        ppp_labels[str(key)] = label_num\n",
    "        label_num+=1\n",
    "    \n",
    "    return ppp_labels\n",
    "\n",
    "PPP_LABELS = build_ppp_labels(NUM_PATCHES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "FRiWmR5NIg3D",
    "outputId": "be0b6368-e84e-4f29-b6a3-e921f40f22f6"
   },
   "outputs": [],
   "source": [
    "#print(PPP_LABELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hamdist(l1,l2):\n",
    "    l1 = list(map(str,list(map(int,l1.strip('[]').split(',')))))\n",
    "    l2 = list(map(str,list(map(int,l2.strip('[]').split(',')))))\n",
    "    \n",
    "    #print(l1)\n",
    "    #print(l2)\n",
    "    \n",
    "    dist = 0\n",
    "    \n",
    "    for i in range(len(l1)):\n",
    "        dist+=int(l1[i]!=l2[i])\n",
    "        \n",
    "    return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hamdist('[1,2,3,4,5,6,7,8,9]','[9,6,5,4,7,2,3,1,8]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "keys = ['[0, 1, 2, 3, 4, 5, 6, 7, 8]']\n",
    "\n",
    "ppplabels = list(PPP_LABELS.keys())\n",
    "\n",
    "for pl in ppplabels:\n",
    "    avg_ham_dist = []\n",
    "    for k in keys:\n",
    "        avg_ham_dist += [hamdist(k,pl)]\n",
    "    all_true = 1\n",
    "    for hd in avg_ham_dist:\n",
    "        if hd < 5:\n",
    "            all_true = 0\n",
    "            \n",
    "    if all_true == 1:\n",
    "        keys.append(pl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ppplabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "C1xoo51m5SFj",
    "outputId": "3f99df5a-a75d-4133-84f9-d6ba01cede9c"
   },
   "outputs": [],
   "source": [
    "keys = random.sample(keys,1000)\n",
    "if '[0, 1, 2, 3, 4, 5, 6, 7, 8]' not in keys:\n",
    "    PPP_LABELS_DICT = {'[0, 1, 2, 3, 4, 5, 6, 7, 8]':0}\n",
    "    keys = keys[:-1]\n",
    "    v = 1\n",
    "else:\n",
    "    PPP_LABELS_DICT = {}\n",
    "    v = 0\n",
    "for k in keys:\n",
    "    PPP_LABELS_DICT[k] = v\n",
    "    v+=1\n",
    "#print(PPP_LABELS_DICT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bfJm6IUNHrVZ"
   },
   "outputs": [],
   "source": [
    "PPP_LABELS = PPP_LABELS_DICT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "fq-2vajXM42Z",
    "outputId": "beb2bece-b7c6-4e58-f92f-b0ea3de2eaf5"
   },
   "outputs": [],
   "source": [
    "'[0, 1, 2, 3, 4, 5, 6, 7, 8]' in PPP_LABELS.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "WSLCxD-rSoeR",
    "outputId": "1f6dab70-d866-4c7d-e51c-ebacceadc2ed"
   },
   "outputs": [],
   "source": [
    "len(PPP_LABELS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f4NSGL_4q0Jo"
   },
   "source": [
    "## SSL PPP Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NrolnQ6_Jjgp"
   },
   "outputs": [],
   "source": [
    "NUM_PATCHES = 9\n",
    "NUM_CLASSES = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fbtQoTWfqy8R"
   },
   "outputs": [],
   "source": [
    "class PPPDataGen(Sequence):\n",
    "    def __init__(self,phase,mode,base_dir,filenames_df,preprocess_input=None,\n",
    "                 ppp_labels_dict = PPP_LABELS,augmentations_dict = augmentations,\n",
    "                 batch_size=8,num_patches = NUM_PATCHES,num_frames = NUM_FRAMES,\n",
    "                 num_classes=NUM_CLASSES,hor_flip = True,data_aug = True):\n",
    "        self.base_dir = base_dir\n",
    "        self.ph_mode_dir = base_dir+'/'+phase+'/'+mode\n",
    "        self.filenames = os.listdir(self.ph_mode_dir)\n",
    "        self.phase = phase\n",
    "        self.mode = mode\n",
    "        self.batch_size = batch_size\n",
    "        self.num_patches = num_patches\n",
    "        self.num_frames = num_frames\n",
    "        self.num_classes = num_classes\n",
    "        self.filenames_df = filenames_df\n",
    "        self.preprocess_input = preprocess_input\n",
    "        self.ppp_labels_dict = ppp_labels_dict\n",
    "        self.augmentations_dict = augmentations_dict \n",
    "        self.hor_flip = hor_flip\n",
    "        self.data_aug = data_aug\n",
    "        \n",
    "        self.invGamma100 = 1.0 \n",
    "        self.invGamma115 = 1.0/1.15\n",
    "        self.invGamma085 = 1.0/0.85\n",
    "        self.table100 = np.array([((k / 255.0) ** self.invGamma100) * 255 for k in np.arange(0, 256)]).astype(\"uint8\")\n",
    "        self.table115 = np.array([((k / 255.0) ** self.invGamma115) * 255 for k in np.arange(0, 256)]).astype(\"uint8\")\n",
    "        self.table085 = np.array([((k / 255.0) ** self.invGamma085) * 255 for k in np.arange(0, 256)]).astype(\"uint8\")\n",
    "        self.gamma_dict100 = dict(zip(range(256),self.table100))\n",
    "        self.gamma_dict115 = dict(zip(range(256),self.table115))\n",
    "        self.gamma_dict085 = dict(zip(range(256),self.table085))\n",
    "\n",
    "\n",
    "    def get_random_shuffle_order(self,batch_sz):\n",
    "        blist = list(range(batch_sz))\n",
    "        random.shuffle(blist)\n",
    "        return blist\n",
    "    \n",
    "    def load_volume(self,mode,file_idx):\n",
    "        filePoolLen = self.filenames_df.shape[0]\n",
    "        file_idx = file_idx%filePoolLen #np.random.randint(0,filePoolLen)\n",
    "        npy_file = np.load(self.ph_mode_dir+'/'+self.filenames_df['filename'].iloc[file_idx])\n",
    "        return npy_file\n",
    "    \n",
    "    def get_frames(self,mode,idx):\n",
    "        image_volume = self.load_volume(mode,idx)\n",
    "        tot_frames = image_volume.shape[0]\n",
    "        frame_idxs = np.random.randint(0,tot_frames,size=self.num_frames)\n",
    "        frames = np.array(image_volume[frame_idxs,:,:])\n",
    "        #print(frames.shape)\n",
    "        return frames\n",
    "    \n",
    "    def __len__(self):\n",
    "        return int(len(self.filenames)) #/np.max(1,int(self.batch_size/self.num_classes)))) #(-1) only for .DS_Store\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        \n",
    "        #self.start_idx = idx*self.batch_size\n",
    "        #self.end_idx = self.start_idx + self.batch_size\n",
    "        \n",
    "        file_idx = idx\n",
    "\n",
    "        #DECLARE VARIABLES\n",
    "        batch_imgs = np.array([]).reshape((0,256,256,3))\n",
    "\n",
    "        model1_inp = np.array([]).reshape((0,64,64,3))\n",
    "        model2_inp = np.array([]).reshape((0,64,64,3))\n",
    "        model3_inp = np.array([]).reshape((0,64,64,3))\n",
    "        model4_inp = np.array([]).reshape((0,64,64,3))\n",
    "        model5_inp = np.array([]).reshape((0,64,64,3))\n",
    "        model6_inp = np.array([]).reshape((0,64,64,3))\n",
    "        model7_inp = np.array([]).reshape((0,64,64,3))\n",
    "        model8_inp = np.array([]).reshape((0,64,64,3))\n",
    "        model9_inp = np.array([]).reshape((0,64,64,3))\n",
    "\n",
    "        batch_labs = np.array([]).reshape((0,self.num_classes))\n",
    "        \n",
    "        #CREATE BATCH\n",
    "        for bs in range(self.batch_size):\n",
    "            #print(bs)\n",
    "            #GET CLIP FRAMES\n",
    "            #file_idx = idx #+ bs\n",
    "            imgs = np.array([]).reshape((256,256,0))\n",
    "            img = self.get_frames(self.mode,idx)\n",
    "            for i in range(3):\n",
    "                imgs = np.append(imgs,img.reshape((256,256,1)),axis=2)\n",
    "            \n",
    "            batch_imgs = np.append(batch_imgs,np.expand_dims(imgs,axis=0),axis=0)\n",
    "\n",
    "        ppp_labels = list(self.ppp_labels_dict.keys())\n",
    "        #print(ppp_labels)\n",
    "        crop_window = int(256/int(np.sqrt(self.num_patches)))\n",
    "        crop_window_rlx = 64 ############################int(crop_window - np.ceil(0.2*float(crop_window)))\n",
    "        eachgap = int((crop_window-crop_window_rlx)/2.)\n",
    "\n",
    "        for i in range(batch_imgs.shape[0]):\n",
    "            if self.phase=='train':\n",
    "                batch_imgs[i] = self.gamma_correction(batch_imgs[i])\n",
    "            \n",
    "            #temp_patch1 = np.zeros(shape = (crop_window_rlx,crop_window_rlx,3))\n",
    "            #temp_patch2 = np.zeros(shape = (crop_window_rlx,crop_window_rlx,3))\n",
    "            label_idx = np.random.choice(list(range(100)))\n",
    "            #print(ppp_labels[label_idx])\n",
    "            jumbling_order = list(map(int,ppp_labels[label_idx].strip('[]').split(',')))\n",
    "            temp_batch_img = np.zeros((64,64,3))\n",
    "            for jo in range(len(jumbling_order)):\n",
    "                temp_patch1 = np.zeros((crop_window_rlx,crop_window_rlx,3))\n",
    "                spatch_num = jumbling_order[jo]\n",
    "                scol_num = spatch_num%int(np.sqrt(self.num_patches))\n",
    "                srow_num = int(np.floor(spatch_num/int(np.sqrt(self.num_patches))))\n",
    "                gapx = np.random.randint(0,eachgap)\n",
    "                gapy = np.random.randint(0,eachgap)\n",
    "                sourcesx = crop_window*scol_num + gapx\n",
    "                sourceex = sourcesx + crop_window_rlx\n",
    "                sourcesy = crop_window*srow_num + gapy\n",
    "                sourceey = sourcesy + crop_window_rlx\n",
    "                #sx = np.random.choice(range(int(eachgap/4.),int(eachgap/4.)+int(eachgap/2.)))\n",
    "                #sy = np.random.choice(range(int(eachgap/4.),int(eachgap/4.)+int(eachgap/2.)))\n",
    "                #temp_patch1 = batch_imgs[i,sourcesx:sourceex,sourcesy:sourceey,:]\n",
    "\n",
    "                #AUGMENT FRAMES\n",
    "                if self.phase=='train':\n",
    "                    temp_patch1 = self.__augment(batch_imgs[i,sourcesx:sourceex,sourcesy:sourceey,:])\n",
    "                else:\n",
    "                    temp_patch1 = batch_imgs[i,sourcesx:sourceex,sourcesy:sourceey,:]\n",
    "\n",
    "                if jo == 0:\n",
    "                    model1_inp = np.append(model1_inp,np.expand_dims(temp_patch1,axis=0),axis=0)\n",
    "                if jo == 1:\n",
    "                    model2_inp = np.append(model2_inp,np.expand_dims(temp_patch1,axis=0),axis=0)\n",
    "                if jo == 2:\n",
    "                    model3_inp = np.append(model3_inp,np.expand_dims(temp_patch1,axis=0),axis=0)\n",
    "                if jo == 3:\n",
    "                    model4_inp = np.append(model4_inp,np.expand_dims(temp_patch1,axis=0),axis=0)\n",
    "                if jo == 4:\n",
    "                    model5_inp = np.append(model5_inp,np.expand_dims(temp_patch1,axis=0),axis=0)\n",
    "                if jo == 5:\n",
    "                    model6_inp = np.append(model6_inp,np.expand_dims(temp_patch1,axis=0),axis=0)\n",
    "                if jo == 6:\n",
    "                    model7_inp = np.append(model7_inp,np.expand_dims(temp_patch1,axis=0),axis=0)\n",
    "                if jo == 7:\n",
    "                    model8_inp = np.append(model8_inp,np.expand_dims(temp_patch1,axis=0),axis=0)\n",
    "                if jo == 8:\n",
    "                    model9_inp = np.append(model9_inp,np.expand_dims(temp_patch1,axis=0),axis=0)\n",
    "\n",
    "            batch_labs = np.append(batch_labs,\n",
    "                                   to_categorical(int(self.ppp_labels_dict[ppp_labels[label_idx]]),self.num_classes).reshape((1,-1)),\n",
    "                                   axis = 0)\n",
    "\n",
    "        #PREPROCESS FRAMES\n",
    "        model1_inp = self.preprocess_input(model1_inp)\n",
    "        model2_inp = self.preprocess_input(model2_inp)\n",
    "        model3_inp = self.preprocess_input(model3_inp)\n",
    "        model4_inp = self.preprocess_input(model4_inp)\n",
    "        model5_inp = self.preprocess_input(model5_inp)\n",
    "        model6_inp = self.preprocess_input(model6_inp)\n",
    "        model7_inp = self.preprocess_input(model7_inp)\n",
    "        model8_inp = self.preprocess_input(model8_inp)\n",
    "        model9_inp = self.preprocess_input(model9_inp)\n",
    "\n",
    "        return [model1_inp,model2_inp,model3_inp,model4_inp,model5_inp,model6_inp,model7_inp,model8_inp,model9_inp],batch_labs\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        self.filenames_df = self.filenames_df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "    def gamma_correction(self,temp_patch):\n",
    "        \n",
    "        gamma_val = np.random.choice([0,1,2])\n",
    "        if gamma_val == 0:\n",
    "            temp_patch = np.vectorize(self.gamma_dict100.get)(temp_patch.astype('int'))\n",
    "        if gamma_val == 1:\n",
    "            temp_patch = np.vectorize(self.gamma_dict115.get)(temp_patch.astype('int'))\n",
    "        if gamma_val == 2:\n",
    "            temp_patch = np.vectorize(self.gamma_dict085.get)(temp_patch.astype('int'))\n",
    "\n",
    "        return temp_patch\n",
    "    \n",
    "    def __augment(self,temp_patch):\n",
    "\n",
    "        transforms = np.random.choice(list(self.augmentations_dict.keys()))\n",
    "        transformations = self.augmentations_dict[transforms]\n",
    "        temp_patch = rotate(temp_patch,transformations[0],preserve_range=True)\n",
    "        temp_patch = warp(temp_patch,\n",
    "                          AffineTransform(matrix=np.array([[transformations[3], 0, transformations[1]],\n",
    "                                                           [0,transformations[3],  transformations[2]],\n",
    "                                                           [0,         0,                   1]])).inverse,\n",
    "                          preserve_range=True)\n",
    "            \n",
    "        if self.hor_flip:\n",
    "            if np.random.choice([True,False]):\n",
    "                temp_patch = np.flip(temp_patch,axis=2)\n",
    "\n",
    "        #if self.gamma:\n",
    "        \n",
    "\n",
    "        return temp_patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0zIXsRxrrB1B"
   },
   "outputs": [],
   "source": [
    "dg = PPPDataGen('train','sagittal',mrnet_path,tr_filenames_df,preprocess_input = ppi_irv2,ppp_labels_dict = PPP_LABELS,augmentations_dict = augmentations,batch_size=16,num_frames = 1,num_classes=NUM_CLASSES,hor_flip = False,data_aug = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "8lf8pVYgrB1J",
    "outputId": "131089f1-3cac-49ae-9e33-b566aa4fbb9b"
   },
   "outputs": [],
   "source": [
    "ppp_imgs,ppp_labs, = dg.__getitem__(1)\n",
    "print(ppp_imgs[0].shape)\n",
    "print(ppp_labs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "XjOURpwXJaRQ",
    "outputId": "7aba4724-711c-4ee5-ffdb-f9a171c8fba5"
   },
   "outputs": [],
   "source": [
    "ppp_labs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jWM4obhErB1H"
   },
   "outputs": [],
   "source": [
    "samples = {}\n",
    "for i in range(1000):\n",
    "    _,gtp_labs = dg.__getitem__(i)\n",
    "    for t in gtp_labs:\n",
    "        if t not in list(samples.keys()):\n",
    "            samples[int(t)]=0\n",
    "        samples[int(t)]+=1\n",
    "plt.bar(list(samples.keys()),list(samples.values()))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 248
    },
    "colab_type": "code",
    "id": "YFnb6kYYPtVQ",
    "outputId": "c1666641-9a41-4421-a98c-ea6514883da2"
   },
   "outputs": [],
   "source": [
    "plt.imshow((ppp_imgs[3]+1)/2.0,cmap='gray')\n",
    "plt.tick_params(\n",
    "    axis='x',          # changes apply to the x-axis\n",
    "    which='both',      # both major and minor ticks are affected\n",
    "    bottom=False,      # ticks along the bottom edge are off\n",
    "    top=False,         # ticks along the top edge are off\n",
    "    labelbottom=False)\n",
    "plt.tick_params(\n",
    "    axis='y',          # changes apply to the x-axis\n",
    "    which='both',      # both major and minor ticks are affected\n",
    "    left=False,      # ticks along the bottom edge are off\n",
    "    right=False,         # ticks along the top edge are off\n",
    "    labelleft=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "JMMhwb01rB1N",
    "outputId": "8532dc9b-759e-4902-d0f5-add201e22e65"
   },
   "outputs": [],
   "source": [
    "fig,axs=plt.subplots(4,4,figsize=(80,80))\n",
    "for i in range(16):\n",
    "    axs[int(i/4),i%4].imshow((ppp_imgs[0][i]+1)/2.0,cmap='gray')\n",
    "    #axs[int(i/4),i%4].set_title(str(list(PPP_LABELS.keys())[ppp_labs[i]]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-D82z4iQNWUm"
   },
   "source": [
    "## Sagittal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wNwfQn-BQo4A"
   },
   "outputs": [],
   "source": [
    "rate = 0.55\n",
    "NUM_CLASSES = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eRZ-ey1_2mPZ"
   },
   "source": [
    "## Manual Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C8PZ2i5F2MSF"
   },
   "outputs": [],
   "source": [
    "def rocket_model(input_shape = (64,64,3)):\n",
    "    model11_inp = Input(shape=input_shape)\n",
    "    model11 = Conv2D(filters = 256, kernel_size = 3, strides = 1, padding = 'same', activation = 'relu',kernel_initializer = tf.keras.initializers.he_normal(seed=16),kernel_regularizer =regularizers.l2(0.0001))(model11_inp)\n",
    "    model11 = Conv2D(filters = 256, kernel_size = 3, strides = 1, padding = 'same', activation = 'relu',kernel_initializer = tf.keras.initializers.he_normal(seed=16),kernel_regularizer =regularizers.l2(0.0001))(model11)\n",
    "    model11 = MaxPool2D(pool_size = (2,2),strides=2)(model11)\n",
    "    model11 = Conv2D(filters = 512, kernel_size = 3, strides = 1, padding = 'same', activation = 'relu',kernel_initializer = tf.keras.initializers.he_normal(seed=16),kernel_regularizer =regularizers.l2(0.0001))(model11)\n",
    "    model11 = Conv2D(filters = 512, kernel_size = 3, strides = 1, padding = 'same', activation = 'relu',kernel_initializer = tf.keras.initializers.he_normal(seed=16),kernel_regularizer =regularizers.l2(0.0001))(model11)\n",
    "    model11 = MaxPool2D(pool_size = (2,2),strides=2)(model11)\n",
    "\n",
    "    model12_inp = Input(shape=input_shape)\n",
    "    model12 = Conv2D(filters = 256, kernel_size = 3, strides = 1, padding = 'same', activation = 'relu',kernel_initializer = tf.keras.initializers.he_normal(seed=16),kernel_regularizer =regularizers.l2(0.0001))(model12_inp)\n",
    "    model12 = Conv2D(filters = 256, kernel_size = 3, strides = 1, padding = 'same', activation = 'relu',kernel_initializer = tf.keras.initializers.he_normal(seed=16),kernel_regularizer =regularizers.l2(0.0001))(model12)\n",
    "    model12 = MaxPool2D(pool_size = (2,2),strides=2)(model12)\n",
    "    model12 = Conv2D(filters = 512, kernel_size = 3, strides = 1, padding = 'same', activation = 'relu',kernel_initializer = tf.keras.initializers.he_normal(seed=16),kernel_regularizer =regularizers.l2(0.0001))(model12)\n",
    "    model12 = Conv2D(filters = 512, kernel_size = 3, strides = 1, padding = 'same', activation = 'relu',kernel_initializer = tf.keras.initializers.he_normal(seed=16),kernel_regularizer =regularizers.l2(0.0001))(model12)\n",
    "    model12 = MaxPool2D(pool_size = (2,2),strides=2)(model12)\n",
    "\n",
    "    model13_inp = Input(shape=input_shape)\n",
    "    model13 = Conv2D(filters = 256, kernel_size = 3, strides = 1, padding = 'same', activation = 'relu',kernel_initializer = tf.keras.initializers.he_normal(seed=16),kernel_regularizer =regularizers.l2(0.0001))(model13_inp)\n",
    "    model13 = Conv2D(filters = 256, kernel_size = 3, strides = 1, padding = 'same', activation = 'relu',kernel_initializer = tf.keras.initializers.he_normal(seed=16),kernel_regularizer =regularizers.l2(0.0001))(model13)\n",
    "    model13 = MaxPool2D(pool_size = (2,2),strides=2)(model13)\n",
    "    model13 = Conv2D(filters = 512, kernel_size = 3, strides = 1, padding = 'same', activation = 'relu',kernel_initializer = tf.keras.initializers.he_normal(seed=16),kernel_regularizer =regularizers.l2(0.0001))(model13)\n",
    "    model13 = Conv2D(filters = 512, kernel_size = 3, strides = 1, padding = 'same', activation = 'relu',kernel_initializer = tf.keras.initializers.he_normal(seed=16),kernel_regularizer =regularizers.l2(0.0001))(model13)\n",
    "    model13 = MaxPool2D(pool_size = (2,2),strides=2)(model13)\n",
    "\n",
    "    model21_inp = Input(shape=input_shape)\n",
    "    model21 = Conv2D(filters = 256, kernel_size = 3, strides = 1, padding = 'same', activation = 'relu',kernel_initializer = tf.keras.initializers.he_normal(seed=16),kernel_regularizer =regularizers.l2(0.0001))(model21_inp)\n",
    "    model21 = Conv2D(filters = 256, kernel_size = 3, strides = 1, padding = 'same', activation = 'relu',kernel_initializer = tf.keras.initializers.he_normal(seed=16),kernel_regularizer =regularizers.l2(0.0001))(model21)\n",
    "    model21 = MaxPool2D(pool_size = (2,2),strides=2)(model21)\n",
    "    model21 = Conv2D(filters = 512, kernel_size = 3, strides = 1, padding = 'same', activation = 'relu',kernel_initializer = tf.keras.initializers.he_normal(seed=16),kernel_regularizer =regularizers.l2(0.0001))(model21)\n",
    "    model21 = Conv2D(filters = 512, kernel_size = 3, strides = 1, padding = 'same', activation = 'relu',kernel_initializer = tf.keras.initializers.he_normal(seed=16),kernel_regularizer =regularizers.l2(0.0001))(model21)\n",
    "    model21 = MaxPool2D(pool_size = (2,2),strides=2)(model21)\n",
    "\n",
    "    model22_inp = Input(shape=input_shape)\n",
    "    model22 = Conv2D(filters = 256, kernel_size = 3, strides = 1, padding = 'same', activation = 'relu',kernel_initializer = tf.keras.initializers.he_normal(seed=16),kernel_regularizer =regularizers.l2(0.0001))(model22_inp)\n",
    "    model22 = Conv2D(filters = 256, kernel_size = 3, strides = 1, padding = 'same', activation = 'relu',kernel_initializer = tf.keras.initializers.he_normal(seed=16),kernel_regularizer =regularizers.l2(0.0001))(model22)\n",
    "    model22 = MaxPool2D(pool_size = (2,2),strides=2)(model22)\n",
    "    model22 = Conv2D(filters = 512, kernel_size = 3, strides = 1, padding = 'same', activation = 'relu',kernel_initializer = tf.keras.initializers.he_normal(seed=16),kernel_regularizer =regularizers.l2(0.0001))(model22)\n",
    "    model22 = Conv2D(filters = 512, kernel_size = 3, strides = 1, padding = 'same', activation = 'relu',kernel_initializer = tf.keras.initializers.he_normal(seed=16),kernel_regularizer =regularizers.l2(0.0001))(model22)\n",
    "    model22 = MaxPool2D(pool_size = (2,2),strides=2)(model22)\n",
    "\n",
    "    model23_inp = Input(shape=input_shape)\n",
    "    model23 = Conv2D(filters = 256, kernel_size = 3, strides = 1, padding = 'same', activation = 'relu',kernel_initializer = tf.keras.initializers.he_normal(seed=16),kernel_regularizer =regularizers.l2(0.0001))(model23_inp)\n",
    "    model23 = Conv2D(filters = 256, kernel_size = 3, strides = 1, padding = 'same', activation = 'relu',kernel_initializer = tf.keras.initializers.he_normal(seed=16),kernel_regularizer =regularizers.l2(0.0001))(model23)\n",
    "    model23 = MaxPool2D(pool_size = (2,2),strides=2)(model23)\n",
    "    model23 = Conv2D(filters = 512, kernel_size = 3, strides = 1, padding = 'same', activation = 'relu',kernel_initializer = tf.keras.initializers.he_normal(seed=16),kernel_regularizer =regularizers.l2(0.0001))(model23)\n",
    "    model23 = Conv2D(filters = 512, kernel_size = 3, strides = 1, padding = 'same', activation = 'relu',kernel_initializer = tf.keras.initializers.he_normal(seed=16),kernel_regularizer =regularizers.l2(0.0001))(model23)\n",
    "    model23 = MaxPool2D(pool_size = (2,2),strides=2)(model23)\n",
    "\n",
    "    model31_inp = Input(shape = input_shape)\n",
    "    model31 = Conv2D(filters = 256, kernel_size = 3, strides = 1, padding = 'same', activation = 'relu',kernel_initializer = tf.keras.initializers.he_normal(seed=16),kernel_regularizer =regularizers.l2(0.0001))(model31_inp)\n",
    "    model31 = Conv2D(filters = 256, kernel_size = 3, strides = 1, padding = 'same', activation = 'relu',kernel_initializer = tf.keras.initializers.he_normal(seed=16),kernel_regularizer =regularizers.l2(0.0001))(model31)\n",
    "    model31 = MaxPool2D(pool_size = (2,2),strides=2)(model31)\n",
    "    model31 = Conv2D(filters = 512, kernel_size = 3, strides = 1, padding = 'same', activation = 'relu',kernel_initializer = tf.keras.initializers.he_normal(seed=16),kernel_regularizer =regularizers.l2(0.0001))(model31)\n",
    "    model31 = Conv2D(filters = 512, kernel_size = 3, strides = 1, padding = 'same', activation = 'relu',kernel_initializer = tf.keras.initializers.he_normal(seed=16),kernel_regularizer =regularizers.l2(0.0001))(model31)\n",
    "    model31 = MaxPool2D(pool_size = (2,2),strides=2)(model31)\n",
    "\n",
    "    model32_inp = Input(shape= input_shape)\n",
    "    model32 = Conv2D(filters = 256, kernel_size = 3, strides = 1, padding = 'same', activation = 'relu',kernel_initializer = tf.keras.initializers.he_normal(seed=16),kernel_regularizer =regularizers.l2(0.0001))(model32_inp)\n",
    "    model32 = Conv2D(filters = 256, kernel_size = 3, strides = 1, padding = 'same', activation = 'relu',kernel_initializer = tf.keras.initializers.he_normal(seed=16),kernel_regularizer =regularizers.l2(0.0001))(model32)\n",
    "    model32 = MaxPool2D(pool_size = (2,2),strides=2)(model32)\n",
    "    model32 = Conv2D(filters = 512, kernel_size = 3, strides = 1, padding = 'same', activation = 'relu',kernel_initializer = tf.keras.initializers.he_normal(seed=16),kernel_regularizer =regularizers.l2(0.0001))(model32)\n",
    "    model32 = Conv2D(filters = 512, kernel_size = 3, strides = 1, padding = 'same', activation = 'relu',kernel_initializer = tf.keras.initializers.he_normal(seed=16),kernel_regularizer =regularizers.l2(0.0001))(model32)\n",
    "    model32 = MaxPool2D(pool_size = (2,2),strides=2)(model32)\n",
    "\n",
    "    model33_inp = Input(shape = input_shape)\n",
    "    model33 = Conv2D(filters = 256, kernel_size = 3, strides = 1, padding = 'same', activation = 'relu',kernel_initializer = tf.keras.initializers.he_normal(seed=16),kernel_regularizer =regularizers.l2(0.0001))(model33_inp)\n",
    "    model33 = Conv2D(filters = 256, kernel_size = 3, strides = 1, padding = 'same', activation = 'relu',kernel_initializer = tf.keras.initializers.he_normal(seed=16),kernel_regularizer =regularizers.l2(0.0001))(model33)\n",
    "    model33 = MaxPool2D(pool_size = (2,2),strides=2)(model33)\n",
    "    model33 = Conv2D(filters = 512, kernel_size = 3, strides = 1, padding = 'same', activation = 'relu',kernel_initializer = tf.keras.initializers.he_normal(seed=16),kernel_regularizer =regularizers.l2(0.0001))(model33)\n",
    "    model33 = Conv2D(filters = 512, kernel_size = 3, strides = 1, padding = 'same', activation = 'relu',kernel_initializer = tf.keras.initializers.he_normal(seed=16),kernel_regularizer =regularizers.l2(0.0001))(model33)\n",
    "    model33 = MaxPool2D(pool_size = (2,2),strides=2)(model33)\n",
    "\n",
    "\n",
    "    model_stem = Concatenate()([model11,model12,model13,model21,model22,model23,model31,model32,model33])\n",
    "    \n",
    "    model_stem = Conv2D(filters = 2048, kernel_size = 3, strides = 1, padding = 'same', activation = 'relu',kernel_initializer = tf.keras.initializers.he_normal(seed=16),kernel_regularizer =regularizers.l2(0.0001))(model_stem)\n",
    "\n",
    "    model_stem1 = Conv2D(filters = 1024, kernel_size = 3, strides = 1, padding = 'same', activation = 'relu',kernel_initializer = tf.keras.initializers.he_normal(seed=16),kernel_regularizer =regularizers.l2(0.0001))(model_stem)\n",
    "    model_stem1 = Conv2D(filters = 1024, kernel_size = 3, strides = 2, padding = 'same', activation = 'relu',kernel_initializer = tf.keras.initializers.he_normal(seed=16),kernel_regularizer =regularizers.l2(0.0001))(model_stem1)\n",
    "\n",
    "    model_stem2 = Conv2D(filters = 1024, kernel_size = 3, strides = 1, padding = 'same', activation = 'relu',kernel_initializer = tf.keras.initializers.he_normal(seed=16),kernel_regularizer =regularizers.l2(0.0001))(model_stem)\n",
    "    model_stem2 = MaxPool2D(pool_size=(2,2),strides=2)(model_stem2)\n",
    "\n",
    "    model_stem12 = Concatenate()([model_stem1,model_stem2])\n",
    "\n",
    "    model_stem12 = GlobalAveragePooling2D()(model_stem12)\n",
    "\n",
    "    model_stem12 = Dense(1024,activation='relu',kernel_initializer = tf.keras.initializers.he_normal(seed=16),kernel_regularizer = regularizers.l2(0.0001))(model_stem12)\n",
    "    model_stem12 = Dense(1024,activation='relu',kernel_initializer = tf.keras.initializers.he_normal(seed=16),kernel_regularizer = regularizers.l2(0.0001))(model_stem12)\n",
    "    output = Dense(1000,activation = 'softmax')(model_stem12)\n",
    "\n",
    "    rocket_model = Model(inputs = [model11_inp,model12_inp,model13_inp,model21_inp,model22_inp,model23_inp,model31_inp,model32_inp,model33_inp],outputs = output,name='rocket_model')\n",
    "    \n",
    "    return rocket_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretext_model = rocket_model((64,64,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "FezOG_ChKDmR",
    "outputId": "584f04d3-fa59-4ba1-b2a4-e65290d063a9"
   },
   "outputs": [],
   "source": [
    "pretext_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 604
    },
    "colab_type": "code",
    "id": "av1PYjzRLGIC",
    "outputId": "1a5955a6-544b-41d8-98df-411112e8a22f"
   },
   "outputs": [],
   "source": [
    "plot_model(rocket_model, to_file='rocket_model.png',show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "id": "pJg-KvM1dJN2"
   },
   "source": [
    "### OPTIMIZER AND DATA GEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "MajczmtGkUJI"
   },
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.RMSprop(tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=0.0001, decay_steps = 1130,decay_rate=0.95,staircase=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "Fh4bR8PlkUJK"
   },
   "outputs": [],
   "source": [
    "tdg = PPPDataGen('train','sagittal',mrnet_path,tr_filenames_df,preprocess_input = ppi_irv2,ppp_labels_dict = PPP_LABELS,augmentations_dict = augmentations,batch_size=32,num_frames=1,num_classes=NUM_CLASSES,hor_flip=False,data_aug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "CFwcd7qSkUJV"
   },
   "outputs": [],
   "source": [
    "vdg = PPPDataGen('valid','sagittal',mrnet_path,val_filenames_df,preprocess_input = ppi_irv2,ppp_labels_dict = PPP_LABELS,augmentations_dict = augmentations,batch_size=32,num_frames=1,num_classes=NUM_CLASSES,hor_flip=False,data_aug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pretext_model.compile(optimizer = optimizer,\n",
    "                     loss = tf.keras.losses.CategoricalCrossentropy(),\n",
    "                     metrics = tf.keras.metrics.CategoricalAccuracy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pretext_model.fit(tdg, epochs = 5, callbacks = get_callbacks('pretext','categorical'), validation_data = vdg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downstream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretext_model = rocket_model((256,256,3))\n",
    "pretext_model.load_weights('/saved_models/sagittal_pretext_best_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for l in pretext_model.layers:\n",
    "    print(l.name, l.output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretext_out1 = pretext_model.get_layer('max_pooling2d_1').output\n",
    "pretext_out2 = pretext_model.get_layer('max_pooling2d_3').output\n",
    "pretext_out3 = pretext_model.get_layer('max_pooling2d_5').output\n",
    "pretext_out4 = pretext_model.get_layer('max_pooling2d_7').output\n",
    "pretext_out5 = pretext_model.get_layer('max_pooling2d_9').output\n",
    "pretext_out6 = pretext_model.get_layer('max_pooling2d_11').output\n",
    "pretext_out7 = pretext_model.get_layer('max_pooling2d_13').output\n",
    "pretext_out8 = pretext_model.get_layer('max_pooling2d_15').output\n",
    "pretext_out9 = pretext_model.get_layer('max_pooling2d_17').output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretext_out = Concatenate(axis=0)([pretext_out1, pretext_out2, pretext_out3,\n",
    "                                   pretext_out4, pretext_out5, pretext_out6,\n",
    "                                   pretext_out7, pretext_out8, pretext_out9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretext_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disc1 = Conv2D(filters = 512, kernel_size = 3, strides = 1, padding = 'same', activation = 'relu',\n",
    "               kernel_initializer = tf.keras.initializers.he_normal(seed=16),\n",
    "               kernel_regularizer = regularizers.l2(0.0001))(pretext_out)\n",
    "\n",
    "disc1 = Conv2D(filters = 512, kernel_size = 3, strides = 2, padding = 'same', activation = 'relu',\n",
    "               kernel_initializer = tf.keras.initializers.he_normal(seed=16),\n",
    "               kernel_regularizer = regularizers.l2(0.0001))(disc1)\n",
    "\n",
    "disc2 = Conv2D(filters = 1024, kernel_size = 3, strides = 1, padding = 'same', activation = 'relu',\n",
    "               kernel_initializer = tf.keras.initializers.he_normal(seed=16),\n",
    "               kernel_regularizer = regularizers.l2(0.0001))(disc1)\n",
    "\n",
    "disc2 = Conv2D(filters = 1024, kernel_size = 3, strides = 2, padding = 'same', activation = 'relu',\n",
    "               kernel_initializer = tf.keras.initializers.he_normal(seed=16),\n",
    "               kernel_regularizer = regularizers.l2(0.0001))(disc2)\n",
    "\n",
    "disc3 = Conv2D(filters = 1024, kernel_size = 3, strides = 1, padding = 'same', activation = 'relu',\n",
    "               kernel_initializer = tf.keras.initializers.he_normal(seed=16),\n",
    "               kernel_regularizer = regularizers.l2(0.0001))(disc2)\n",
    "\n",
    "disc3 = Conv2D(filters = 1024, kernel_size = 3, strides = 2, padding = 'same', activation = 'relu',\n",
    "               kernel_initializer = tf.keras.initializers.he_normal(seed=16),\n",
    "               kernel_regularizer = regularizers.l2(0.0001))(disc3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gap = GlobalAveragePooling2D()(disc3)\n",
    "\n",
    "maxoverframes = tf.keras.layers.Lambda(lambda x : tf.keras.backend.max(x,axis=0,keepdims = True))(gap)\n",
    "\n",
    "fc1 = Dense(1024, activation='relu',kernel_initializer = tf.keras.initializers.he_normal(seed=16),\n",
    "            kernel_regularizer = regularizers.l2(0.0001))(maxoverframes)\n",
    "\n",
    "out = Dense(1, activation = 'sigmoid',kernel_initializer = tf.keras.initializers.he_normal(seed=16),\n",
    "            kernel_regularizer = regularizers.l2(0.0001))(fc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsmodel = Model(inputs = pretext_model.input , outputs = out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsmodel.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OVERSAMPLING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_1 = np.count_nonzero(tr_multilabel['acl']==1)\n",
    "NUM_0 = np.count_nonzero(tr_multilabel['acl']==0)\n",
    "min_class = np.argmin(np.array([NUM_0,NUM_1]))\n",
    "if int(min_class) == 0:\n",
    "    gapnum = (1130-NUM_0)-NUM_0\n",
    "    INDICES = tr_multilabel[tr_multilabel['acl']==0].index.values\n",
    "    INDICES = np.random.choice(list(INDICES),gapnum)\n",
    "else:\n",
    "    gapnum = (1130-NUM_1)-NUM_1\n",
    "    INDICES = tr_multilabel[tr_multilabel['acl']==1].index.values\n",
    "    INDICES = np.random.choice(list(INDICES),gapnum)\n",
    "    \n",
    "tr_acl_multilabel = tr_multilabel.append(tr_multilabel.iloc[INDICES,:],ignore_index=True)\n",
    "tr_acl_filenames_df = tr_filenames_df.append(tr_filenames_df.iloc[INDICES,:],ignore_index=True)\n",
    "\n",
    "print(tr_acl_multilabel)\n",
    "print(tr_acl_filenames_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downstream Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DSDataGen(Sequence):\n",
    "    def __init__(self, phase, base_dir, labs_df, filenames_df, injury, preprocess_input = None,batch_size=8, max_batch_size = 32, data_aug = True, num_frames = NUM_FRAMES, num_classes=NUM_CLASSES):\n",
    "        self.base_dir = base_dir\n",
    "        self.ph_mode_dir = base_dir+'/'+phase\n",
    "        self.filenames = os.listdir(self.ph_mode_dir)\n",
    "        self.phase = phase\n",
    "        self.batch_size = batch_size\n",
    "        self.num_frames = num_frames\n",
    "        self.num_classes = num_classes\n",
    "        self.filenames_df = filenames_df\n",
    "        self.preprocess_input = preprocess_input\n",
    "         \n",
    "        self.mode = ['sagittal','coronal','axial']\n",
    "        self.injury = injury\n",
    "\n",
    "        self.mllabs = labs_df\n",
    "\n",
    "        self.indices = list(range(self.filenames_df.shape[0]))\n",
    "        \n",
    "        self.data_aug = data_aug\n",
    "\n",
    "\n",
    "    def get_random_shuffle_order(self,batch_sz):\n",
    "        blist = list(range(batch_sz))\n",
    "        random.shuffle(blist)\n",
    "        #print(blist)\n",
    "        return blist\n",
    "    \n",
    "    def load_volume(self,mode,file_idx):\n",
    "        filePoolLen = self.filenames_df.shape[0]\n",
    "        #print(file_idx)\n",
    "        file_idx = file_idx%filePoolLen \n",
    "        npy_file = np.load(self.ph_mode_dir+'/'+mode+'/'+self.filenames_df['filename'].iloc[file_idx])\n",
    "        return npy_file\n",
    "    \n",
    "    def get_frames(self,mode,idx):\n",
    "        image_volume = self.load_volume(mode,self.indices[idx])\n",
    "        tot_frames = image_volume.shape[0]\n",
    "        #print(tot_frames)\n",
    "        #print(mode,tot_frames)\n",
    "        self.num_frames = min([self.num_frames, tot_frames])\n",
    "        sampling_interval = int(tot_frames/self.num_frames)\n",
    "        \n",
    "        nf_mid = int(self.num_frames/2)\n",
    "        nf_lr = int(nf_mid/2)\n",
    "        left_sec_end = int(tot_frames/2) - int(tot_frames/4)\n",
    "        right_sec_start = int(tot_frames/2) + int(tot_frames/4)\n",
    "\n",
    "        left_frames = np.array(sorted(random.sample(range(left_sec_end),nf_lr)))\n",
    "        right_frames = np.array(sorted(random.sample(range(right_sec_start,tot_frames),nf_lr)))\n",
    "        mid_frames = np.array(sorted(random.sample(list(range(left_sec_end,left_sec_end+nf_mid)),nf_mid)))\n",
    "        #print(left_frames,mid_frames,right_frames)\n",
    "        frame_idxs = np.append(np.append(left_frames,mid_frames),right_frames)\n",
    "        #print(frame_idxs)\n",
    "        \n",
    "        #frame_idxs = sorted(random.sample(list(range(tot_frames)),self.num_frames))\n",
    "\n",
    "        frames = np.array([]).reshape((0,256,256,3))\n",
    "        for n in range(frame_idxs.shape[0]):\n",
    "            frame_idx = frame_idxs[n] #np.random.randint(n*sampling_interval,(n+1)*sampling_interval,size=1)\n",
    "            frame = np.array(image_volume[[frame_idx],:,:])\n",
    "            frame = np.expand_dims(frame,axis=3)\n",
    "            frame = np.append(frame,np.append(frame,frame,axis=3),axis=3)\n",
    "            frames = np.append(frames,frame,axis=0)\n",
    "        #print(frames.shape)\n",
    "        return frames\n",
    "    \n",
    "    def __len__(self):\n",
    "        return int(np.floor((len(self.filenames_df))/self.batch_size))\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        \n",
    "        #DECLARE VARIABLES\n",
    "        sagittal_batch_imgs = np.array([]).reshape((0,256,256,3))\n",
    "        \n",
    "        ds_batch_labs = np.array([]).reshape((0,1))\n",
    "        \n",
    "        #CREATE BATCH\n",
    "        for bs in range(self.batch_size):\n",
    "            #GET CLIP FRAMES\n",
    "            sagittal_batch_imgs = np.append(sagittal_batch_imgs,self.get_frames('sagittal',idx),axis=0)\n",
    "            \n",
    "            ds_batch_labs = self.mllabs[self.injury].iloc[self.indices[idx]].reshape((1,-1))\n",
    "\n",
    "        #print(batch_imgs.shape)\n",
    "\n",
    "        #AUGMENT FRAMES\n",
    "        if self.data_aug:\n",
    "            sagittal_batch_imgs = self.__augment(sagittal_batch_imgs)\n",
    "\n",
    "        #PREPROCESS FRAMES\n",
    "        sagittal_batch_imgs = self.preprocess_input(sagittal_batch_imgs)\n",
    "        \n",
    "        inputs = []\n",
    "        numf = self.num_frames//9\n",
    "        #print(self.num_frames,numf)\n",
    "        for i in range(9):\n",
    "            s = i*numf\n",
    "            e = (i+1)*numf\n",
    "            inputs.append(sagittal_batch_imgs[s:e])\n",
    "        \n",
    "        return inputs, ds_batch_labs\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        random.shuffle(self.indices)\n",
    "    \n",
    "    def __augment(self,batch_imgs):\n",
    "        num_imgs = batch_imgs.shape[0]\n",
    "        rotang = np.random.choice([-30,0,30])\n",
    "        scale = np.random.choice([1,1.2])\n",
    "        transformation_matrix=np.array([[scale,           0,             np.random.choice([-25,0,25])],\n",
    "                                        [0,               scale,         np.random.choice([-25,0,25])],\n",
    "                                        [0,               0,                       1                 ]])\n",
    "        for i in range(num_imgs):\n",
    "            batch_imgs[i] = rotate(batch_imgs[i],rotang,preserve_range=True)\n",
    "            batch_imgs[i] = warp(batch_imgs[i], AffineTransform(matrix=transformation_matrix).inverse, preserve_range=True)\n",
    "            #print(batch_labs)\n",
    "                \n",
    "        return batch_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdg = DSDataGen('train',mrnet_path,tr_acl_multilabel,tr_acl_filenames_df,'acl',\n",
    "                preprocess_input = ppi_irv2,batch_size=1,data_aug = True,num_frames=36,num_classes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vdg = DSDataGen('valid',mrnet_path,val_multilabel,val_filenames_df,'acl',\n",
    "                preprocess_input = ppi_irv2,batch_size=1,data_aug = False,num_frames=36,num_classes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsopt = tf.keras.optimizers.Adam(0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsmodel.compile(optimizer = dsmodel.optimizer,\n",
    "               loss = tf.keras.losses.BinaryCrossentropy(),\n",
    "               metrics = [tf.keras.metrics.BinaryAccuracy(),tf.keras.metrics.AUC()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dshist = dsmodel.fit(tdg, epochs = 30, validation_data = vdg, callbacks = get_callbacks('downstream','binary'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsmodel = tf.keras.models.load_model('/saved_models/sagittal_downstream_best_model.h5',compile=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.array([]).reshape((0,1))\n",
    "labs = np.array([]).reshape((0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(val_filenames_df)):\n",
    "    inps, lab = vdg.__getitem__(i)\n",
    "    pred = dsmodel(inps)\n",
    "    preds = np.append(preds,pred.numpy().reshape((-1,1)),axis=0)\n",
    "    labs = np.append(labs,lab,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perf_df = get_performance_metrics(labs,preds,['acl'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perf_df"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "9Y2cdtkeps4I",
    "9E8t_MKiTlz6",
    "ZMBxyH8xiQLM",
    "odE4wSWawzrC",
    "flX-g831ijCK",
    "9XSAi9t4kUHj",
    "w2W1BmnmjfjZ",
    "RLjo17s2kUHx",
    "iNsgmvb3kyuQ",
    "qQJUoL4zuNYJ",
    "P3YXuO9PkUIa",
    "aHUjjDA9PDWi",
    "WPeQvoABkUIh",
    "vrRCO2yLkUIi",
    "qWlqgkNqdpOV",
    "K8Ot_HnjkUI7",
    "8iZOEKBNR47r",
    "K-6v-4P4R9KD",
    "F7Eq24j1R_LD",
    "2w2d34f2aieH",
    "Vfoe6-wZdGYr",
    "l0iA4BE9NecE",
    "pbhZ74ihv1tZ",
    "TpkiX2-NNne2",
    "HIK1G6Cyv_4A"
   ],
   "name": "ROCKET MODEL.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "625px",
    "width": "382px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
